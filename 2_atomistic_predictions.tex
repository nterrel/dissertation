\chapter{Exploration of atomistic predictions}
\label{chapter2}

Atomic interactions are the foundation on which many ML models---especially interatomic potential models---are built.
Accuracy is well-defined in quantum chemical methods via systematic convergence and comparison to experimental data, but wavefunction-based approaches are computationally expensive and are limited to system sizes of only a handful of atoms.
Measuring predictive accuracy is difficult when approximating the results of first principles computations for systems that exceed the size limitations of QM approaches.
This raises a vital question to the efficacy of ML models: how do we assess the reliability of atomistic NNP models for systems that exceed the size limitations of QM computations?

The scalability of NNP models stems from partitioning molecules into atomic components, characterized by their local chemical environment.
However, this introduces new challenges in generalization and transferability and, therefore, model reliability when exposed to new atomic interactions during inference.
This chapter serves as an assessment of the reliability of model predictions of molecular enery and atomistic energy contributions in ANAKIN-ME neural network potential models.
Section \ref{sec:ANI_predictions} contains a detailed look at the predictions of ANI neural networks from atomistic features to global properties and the measure of uncertainty used to determine the trustworthiness of a prediction.
The following section, \ref{sec:uncertainty_atomic_energies}, details the search for an improved metric of reliability in atomistic predictions.

\section{ANAKIN-ME Predictions}
\label{sec:ANI_predictions}

The ANI methodology provides a framework for atom-wise decomposition of molecular energies by using the atomic environment vector (AEV) representation to encode localized chemical interactions.
A decomposition approach allows for efficient, scalable predictions of energy for systems larger than the training data represents.
This is advantageous for simulating large systems, due to the size limitations of \textit{ab initio} computations.
The driving idea behind the AEV representation is that all atomic interactions within a system can be characterized by the localized environment. 
This neglects long-range interactions such as van der Waals forces and electrostatics, which are undoubtedly important for some systems, but has little influence on the predictive accuracy of single-point calculations of small organic molecules.
Corrective approaches to long-range interactions have been explored, such as ANI-MBIS-q \cite{ml_mm_santi_y_jonny2}, which considers long-range electrostatics using a separate, parallel network to predict partial charges.
For small, charge-neutral molecules, localized interactions encoded in the AEV have been shown to capture the relationship between molecular energy and atomic species and their positions \cite{ani-1, ani-1x, ani-2x}.

Though some partitioning schemes exist in quantum theory, such as Bader's Atoms in Molecules \cite{bader_aim}, there is no general consensus for decomposing the energy of a molecular system into a sum of atomic contributions.
The energy decomposition approach taken by ANI is a learned relationship decided by the individual atomic neural networks during training, explained in greater detail in Subsection \ref{subsec:total_E_sum_AEs}.
The uncertainty measure of predicted energies, referred to as the QBC, is defined in Subsection \ref{subsec:ANI_uncertainty}, followed by a case study in Subsection \ref{subsec:flaws_in_qbc} of the reliability of the QBC as a measure of empirical uncertainty in ANI predictions.

\subsection{Molecular Energy as a Sum of Atomic Contributions}
\label{subsec:total_E_sum_AEs}

The predicted molecular energy for a molecule input to an ANI model is computed as a sum of atomic contributions, given in Eqn. \ref{eq:total_E_sum_AEs}, where $E_{\text{Total}}$ is the molecular energy prediction and $\varepsilon_i$ is the atomic energy contribution for atom $i$.

\begin{equation}
    E_{\text{Total}} = \sum_{i}^{\text{atoms}} \varepsilon_i
    \label{eq:total_E_sum_AEs}
\end{equation}

To visualize atomic energy predictions, a small testing subset was created using the first conformer of each molecular formula in the ANI-1x training dataset.
This testing set, referred to as 1x-test ($N=$ 3,114), is used to explore some trends in predicted atomic energies for the carbon, hydrogen, nitrogen, and oxygen atom types across an ensemble of eight models.
Testing on this subset gives insight on the performance of ANI models for well-characterized systems, as this was the data used in training the model.
The ANI-1x and 2x potential models were trained using self-atomic energies (SAEs), where the predicted atomic energy values form a distribution centered at zero. 
This means that, by definition, each atom has either no net contribution to the molecular energy, or contributes an energy deviation that is relative to the baseline for that atom type.
The atomic energy contributions predicted by the ANI-2x model for C, H, N, and O atoms across the 1x-first test set are given in Figure \ref{fig:2x_ae_per_model}.

\begin{figure}[!h]
    \centering
    \includegraphics[width=1\linewidth]{Images/2x_outputs/2x_1x-first_ae-per-model.png}
    \caption[Atomic energies predicted by ANI-2x]{Per-model distribution of atomic energy contributions by atom type for the ANI-2x potential. Values shown in kcal/mol for HCNO atom types on the 1x-first subset\\($N_\text{H}=$ 36,329; 
    $N_\text{C}=$ 21,907; 
    $N_\text{N}=$ 8,211;
    $N_\text{O}=$ 6,303).}
    \label{fig:2x_ae_per_model}
\end{figure}

Shifting atomic energies with SAEs causes the model to learn relative energy contributions rather than absolute values. 
Refined ANI models, such as ANI-2xr, are trained with slightly different hyperparameters.
The largest change between ANI-2x and 2xr is the removal of biases from neuron outputs; Eqn. \ref{eq:nn_eqn} shows the method for computing the weighted-sum output of each neuron ($y$) from the input ($x_i$) multiplied by its weight ($w_i$).
Traditionally, artificial neural networks use biases to output some value, even when the weighted sum of input values is close or equal to zero.
For interatomic potentials, if we pass a single atom with no neighbors through a network, we would expect to get exactly the energy of that atom in space with no other interacting particles.
By removing the biases from the species-specific networks, we can achieve this goal and add a bit more physicality to the behavior of the neural network outputs.
To replace the biases, we add a final shift to the output: the EnergyShifter.
This is a protocol within TorchANI which adds the ground state atomic energy (GSAE) to each atom, shifting the output values of the NNs toward physically meaningful values.
Ground state atomic energies are calculated at the level of theory used in producing training data from a neutrally charged atom isolated in space with the proper spin multiplicity (ensuring a ground-state electronic configuration).
Thus, the total energy prediction in ANI-2xr networks are computed as shown in Equation \ref{eq:total_E_GSAEs}.

\begin{equation}
    E_{\text{Total}} = 
    \sum_{i}^{\text{atoms}} 
    \left(
    \varepsilon_i + \text{GSAE}_i
    \right)
    \label{eq:total_E_GSAEs}
\end{equation}

Here, the total energy is a sum of atomic contributions as in Eqn. \ref{eq:total_E_sum_AEs}, and $\text{GSAE}_i$ is the ground state atomic energy for atom $i$, which shift the network outputs toward more physically meaningful values.
The ground state atomic energies for each atom in the ANI networks are given in Appendix \ref{appendix:GSAEs}.
In addition to the removal of biases and the replacement of self-atomic energies with GSAEs, ANI-2xr is trained with the addition of a simple, analytical xTB repulsion term \cite{xtb_repulsion}.
The inclusion of repulsion to the potential introduces more physical behavior in regions of the potential energy surface that are difficult to sample, namely when two atoms get very close together.
Equation \ref{eq:repulsion} shows the form of this repulsion term for two atoms, where $a$ and $b$ (with atomic numbers $A$ and $B$, respectively) separated by a distance $r_{ab}$. 
The parameters $Y_A$ and $Y_B$ define the magnitude of the repulsive term for their respective elements, $\alpha_{A}$ and $\alpha_{B}$ are element-specific parameters, and $k_{AB}$ is a small correction for very light elements (H, He) and equal to 3/2 in all other cases.

\begin{equation}
\label{eq:repulsion}
    E_{\text{rep}}(r_{ab}) = 
    \sum_{AB}\frac{{Y_{A} Y_{B}}}{r_{ab}}
    \exp \left( -\sqrt{\alpha_{A} \alpha_{B}} {(r_{ab})}^{k_{AB}} \right)
\end{equation}

The ANI-2xr atomic energy distribution for hydrogen, carbon, nitrogen, and oxygen are given in Figure \ref{fig:2xr_ae_per_model} for an eight-membered ensemble.
Note here that the distributions have slightly different shapes, as well as being shifted far from the zero-centered ANI-2x distributions shown in Figure \ref{fig:2x_ae_per_model} above.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{Images/2xr_outputs/2xr_1x-first_ae-per-model.png}
    \caption[Atomic energies predicted by ANI-2xr]{Distribution of atomic energy contributions by atom type for the ANI-2xr potential. Values shown in kcal/mol for CHNO atom types on the 1x-first subset  ($N_\text{H}=$ 36,329; 
    $N_\text{C}=$ 21,907; 
    $N_\text{N}=$ 8,211;
    $N_\text{O}=$ 6,303).}
    \label{fig:2xr_ae_per_model}
\end{figure}

The difference in the shapes of these distributions is an important aspect of analyzing the predictive uncertainty of ANI neural network potentials.
Due to the differences in hyperparameters used in training, such as the removal of biases, the use of the GELU activation function \cite{gelu}, continuous second derivatives improving the smoothness of predicted potential energy surfaces, and the addition of ground state atomic energies and a repulsive term, ANI-2xr will be used to explore trends in predictive uncertainty by ANI neural networks. 

\subsection{Uncertainty in ANI Neural Network Potentials}
\label{subsec:ANI_uncertainty}

ANI potential models use an ensemble of eight independently trained models, allowing for the estimation of uncertainty in energy predictions.
This metric is frequently used in active learning strategies to guide data selection for model improvement.
Predictive uncertainty has been measured in published models using $\hat{\rho}$; the value 0.23 kcal/mol was empirically chosen for sampling structures with high-error energy predictions via query by committee (QBC) in the active learning process \cite{ani-1x}.
The QBC value, given in Eqn. \ref{eq:energy_qbc}, can be thought of as a binary classifier: molecules with QBC values above 0.23 kcal/mol are highly-uncertain predictions and shouldn't be trusted, while values below 0.23 kcal/mol are trustworthy, agreed upon values by the ensemble.

\begin{equation}
\rho = \frac{\sigma_{E_{\text{Total}}}}{\sqrt{N_{\text{atoms}}}}
\label{eq:energy_qbc}
\end{equation}

This value is normalized by the size of the molecule, ensuring that uncertainty values remain comparable across different molecular systems. 
In the development of ANI-1x, the threshold value of 0.23 kcal/mol was empirically chosen as the criterion for selecting molecules with high-energy uncertainty for further active learning \cite{ani-1x}.
However, while the energy QBC serves as a useful uncertainty metric for guiding active learning, it introduces a size-dependent limitation. 
Because $\sqrt{N_\text{atoms}}$ appears in the denominator, molecules with a large number of atoms can exhibit low uncertainty values, even if individual atomic energy predictions are highly uncertain---particularly in unique structural motifs. 
This means that large structures may be misclassified as “low-error” molecules despite containing regions of high uncertainty. 
Consequently, high-error structures may slip through the active learning process, leading to gaps in the training data for larger molecules.

To explore this issue further, we examine atomic energy predictions on a per-atom-type basis. 
Figure \ref{fig:2xr_comp6v1_mean-ae-per-atomtype} presents the distribution of mean atomic energy predictions using the ANI-2xr model on the COMP6v1 benchmark set. 
This per-atom decomposition allows for a finer-grained analysis of uncertainty, revealing how specific atomic environments contribute to total energy predictions and where potential weaknesses in the model may arise.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{Images/2xr_outputs/2xr_comp6v1_mean-ae-per-atomtype.png}
    \caption[Mean atomic energy prediction per-atom with ANI-2xr]{Distribution of predicted mean atomic energies using the ANI-2xr model on the COMP6v1 benchmark set ($N_\text{H}=$ 1,352,100; 
    $N_\text{C}=$ 890,691; 
    $N_\text{N}=$ 193,592;\\ 
    $N_\text{O}=$ 172,475).}
    \label{fig:2xr_comp6v1_mean-ae-per-atomtype}
\end{figure}

By analyzing per-atom uncertainty estimates, it becomes possible to refine uncertainty quantification methods beyond the QBC, addressing the size-dependent bias and improving the selection of high-error molecules for retraining in future ANI models.

\subsection{Flaws in Measuring Uncertainty via Predicted Total Energy}
\label{subsec:flaws_in_qbc}

Shown in Equation \ref{eq:energy_qbc}, the uncertainty metric historically used in the ANI methodology is the energy QBC value. 
This metric is flawed in that it is extremely size-dependent and can be skewed by a large number of atoms within a system. 
Molecules with a high QBC value are more likely to be smaller, less chemically complex molecules, which can lead to sampling biases in active learning.
A clear example of size dependence in ANI uncertainty estimation can be illustrated by comparing the QBC values of two molecules with similar structural motifs but very different $N_{\text{atoms}}$, as shown in Figure \ref{fig:qbc_size_dep}.

\begin{flushleft}
\begin{multiFigure}
    \addFigure{0.5}{Images/qbc_size_dep/m-epoxide-cropped.png}
    \addFigure{0.5}{Images/qbc_size_dep/long-epoxide-cropped.png} \\
    \addFigure{0.5}{Images/qbc_size_dep/m-epox-2d.png}
    \addFigure{0.5}{Images/qbc_size_dep/l-epox-2d.png}
\captionof{figure}[Size-dependence of QBC]{
(A) Example structure with a high QBC.
(B) A molecule with the same geometry in the ring, but an extended hydrocarbon side chain. 
Subfigures (C) and (D) show 2D line structures of these molecules.
}
\label{fig:qbc_size_dep}
\end{multiFigure}
\end{flushleft}

Suppose we compute the ANI energy of a simple, small molecule, such as C\textsubscript{6}H\textsubscript{10}O shown in Figure \ref{fig:qbc_size_dep} (A), where a particular atomic region---here, the epoxide ring---exhibits a relatively high uncertainty in its predicted energy. 
Because QBC is normalized by the size of the system, the uncertainty estimate is influenced heavily by molecular size.
Consider a similarly structured molecule, C\textsubscript{19}H\textsubscript{36}O, with a long alkyl chain replacing the methyl side chain. 
Both systems were minimized using the ANI potential with the ASE \cite{ASE_paper} interface, and the exact same geometry for the six core heavy atoms with their associated hydrogen atoms was used in both structures.
The system sizes and QBC values for these two structures are presented in Table \ref{tbl:qbc_size_dep}.

\begin{table}[ht]
    \begin{centering}
    \setlength{\tabcolsep}{11pt}
    \caption[Size dependence of the QBC]{Size dependence of the QBC uncertainty metric for two similar structures with different $N_{\text{atoms}}$, shown in Figure \ref{fig:qbc_size_dep}.}
    \begin{tabularx}\textwidth{llrr}
    \toprule
    Molecular formula & SMILES & $\sqrt{N_{\text{atoms}}}$ & $\rho$ (kcal/mol) \\
    \midrule
    C\textsubscript{6}H\textsubscript{10}O & (CC1CC2OC2C1) & 4.12 & 0.2312  \\
    C\textsubscript{19}H\textsubscript{36}O & (CCCCCCCCCCCCCCC1CC2OC2C1) & 7.48 & 0.0964  \\
    \bottomrule
    \end{tabularx}
    \label{tbl:qbc_size_dep}
    \end{centering}
\end{table}

Despite including the same structural motifs contributing to a highly uncertain energy prediction and due to the much larger value of $N_{\text{atoms}}$, we see a QBC value of just 0.0946 kcal/mol for structure (B) while structure (A) falls right on the cutoff for trustworthy predictions of 0.23 kcal/mol.
Because $\sqrt{N_\text{atoms}}$ is much larger in the system with an extended alkyl side chain, the total molecular uncertainty value ($\rho$) is lower, even though the specific region which produces highly uncertain energy predictions has not changed.
This example demonstrates how large molecules can exhibit deceptively low QBC values, making it more difficult to identify high-error structures in active learning. 
As a result, potentially problematic regions of uncertainty can be overlooked in training set refinement, leading to overconfidence in predictions for large systems. 
This highlights the need for alternative uncertainty quantification methods that account for localized atomic errors rather than relying solely on global molecular uncertainty metrics.

As the energy of molecules is predicted as a sum of atomic contributions---therefore, one could say, the networks are actually predicting atomic energies in inference---the first exploration into an atomistic uncertainty method was into the atomic energy contribution predicted for each atom.

\section{Uncertainty of Atomic Energy Predictions}
\label{sec:uncertainty_atomic_energies}

Predictive uncertainty in ANI neural network potentials extends beyond total molecular energy predictions, and can be measured across atomic energy contributions on a per-atom basis. 
The standard deviation in atomic energy predictions across an ensemble of ANI models was believed to provide some insight into how confidently the model assigns energetic contributions to individual atoms, that higher uncertainty in atomic energy predictions correlated with underrepresented chemical environments in the training data where the model struggles to generalize beyond learned patterns. 
This would mean that atoms in highly strained or reactive regions would tend to exhibit greater variance in predicted energy contributions, reflecting the challenge of accurately capturing energy contributions in chemically complex systems.
Figure \ref{fig:2xr_comp6v1_stdev-ae-per-atomtype} illustrates the distribution of atomic energy standard deviations for the H, C, N, O atom types using the ANI-2xr model on the COMP6v1 benchmark set.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{Images/2xr_outputs/2xr_comp6v1_stdev-ae-per-atomtype.png}
    \caption[Standard deviation in predicted atomic energy contribution for H, C, N, O atom types]{Distribution of standard deviation in predicted atomic energies using the ANI-2xr model on the COMP6v1 benchmark set ($N_\text{H}=$ 1,352,100; 
    $N_\text{C}=$ 890,691;\\ 
    $N_\text{N}=$ 193,592; 
    $N_\text{O}=$ 172,475).}
    \label{fig:2xr_comp6v1_stdev-ae-per-atomtype}
\end{figure}

The standard deviation of atomic energy contribution varies from species to species, in the shape and the breadth of the distribution for each atom type.
Higher uncertainty in atomic energy contributions arises from the complexity of possible chemical environments and the representation of those environments in the training data.
Note in Figure \ref{fig:2xr_comp6v1_stdev-ae-per-atomtype} that nitrogen and oxygen atoms typically exhibit lower standard deviations in predicted atomic energy, this is due to the limited patterns of bonding which need to be learned by ANI throughout training in order to represent the behavior of those atom types.
Due to the versatility of carbon in organic compounds, we unsurprisingly see the most diverse distribution for this atom type, seeming to be composed of a few overlapping regions of uncertainty. 
Hydrogen, on the other hand, only makes one type of bond, but it can experience very different degrees of polarization depending on what it is bonded to. 
An illustrative example of this phenomenon can be seen in methane (CH\textsubscript{4}). 
Figure \ref{fig:ch4_stdev_ae} presents the standard deviation in atomic energy contributions for CH\textsubscript{4} across the ANI-2xr model ensemble, compared to the total energy uncertainty distribution in the 1x-first test set.

\begin{flushleft}
\begin{multiFigure}
    \addFigure{0.31}{Images/ch4.png}
    \addFigure{0.68}{Images/ch4_example/ch4_2xr_total-e-qbc.png} \\
    \addFigure{1}{Images/ch4_example/ch4_2xr_stdev-ae.png}
\captionof{figure}[CH\textsubscript{4} atomic energy distribution versus 1x-first subset]{
(A) Structure of geometry-optimized CH\textsubscript{4}; 
(B) Plot of the distribution of total energy QBC across the 1x-first test set ($N_\text{molecules}=$ 3,114), 
(C) standard deviation in predicted atomic energy for hydrogen (left, $N=36,329$) and carbon (right, $N=21,907$); 
the red lines show the standard deviation in CH\textsubscript{4} total energy (B) and atomic energy contributions (C).
}
\label{fig:ch4_stdev_ae}
\end{multiFigure}
\end{flushleft}

Note that the distributions shown in Figure \ref{fig:ch4_stdev_ae} differ from those in \ref{fig:2xr_comp6v1_stdev-ae-per-atomtype}, as this plots predictive uncertainties from a subset of the ANI-2xr training data.
As this is compared against the training data, one would expect that methane, a molecule that is represented 1,376 times in the training dataset, would have extremely low uncertainties associated with predictions from models trained on this dataset.
While the total molecular energy for CH\textsubscript{4} is predicted with exceptionally low uncertainty $\left( \rho = 0.0775 \ \text{kcal/mol} \right)$, the per-atom standard deviations reveal higher variability in atomic energy contributions, particularly in carbon.
This suggests that even in highly symmetric molecules, such as CH\textsubscript{4}, the per-atom energy assignments predicted by ANI models may be trivial, model dependent values, introducing potential sources of error in energy decomposition analyses.

The per-atom uncertainty in ANI models is not only influenced by the training data but also by learned covariances across network predictions. 
The atomic energy contributions predicted by different models within the ANI ensemble exhibit correlated variations, where changes in one atomic energy may be systematically offset by adjustments in another.
For example, Table \ref{tbl:ch4_AEs} reports the atomic energy contributions predicted by each model (before and after the GSAEs are added) in the ANI-2xr ensemble for CH\textsubscript{4}.

% Tables suck, but this helped me align columns and headers separately:
\begin{table}[h]
\centering
\caption[CH\textsubscript{4} atomic energy contributions per-model]{
Raw atomic energy contributions by atom type in a geometry-optimized molecule of CH\textsubscript{4}, with four equivalent hydrogen atoms. 
Additionally, the sum over all atoms, and that sum after passing through the TorchANI EnergyShifter. 
Values obtained from an ensemble of ANI models (ANI2xr); the last column shows the standard deviation of predictions across the ensemble. 
All values are in kcal/mol.
}\label{tbl:ch4_AEs}
    \begin{tabularx}{\textwidth}{%
    >{\raggedleft\arraybackslash}r  % Numeric
    >{\raggedleft\arraybackslash}r  % Numeric
    >{\raggedleft\arraybackslash}r  % Numeric
    >{\raggedleft\arraybackslash}r  % Numeric
    >{\raggedleft\arraybackslash}r  % Numeric
    }  
\hline
Model & Carbon & Hydrogen  & Sum of atomic energies & Molecular energy \\
\hline
Mean prediction & -199.0069 & -55.1013 & -419.4123 & -25,413.8125 \\
1 & -219.4329 &  -50.0170 &  -419.5012 &  -25,413.9043 \\
2 & -194.7117 &  -56.1991 &  -419.5081 &  -25,413.9102 \\
3 & -192.8823 &  -56.6466 &  -419.4689 &  -25,413.8691 \\
4 & -201.5683 &  -54.4451 &  -419.3490 &  -25,413.7500 \\
5 & -196.5227 &  -55.7060 &  -419.3471 &  -25,413.7500 \\
6 & -212.9323 &  -51.5945 &  -419.3103 &  -25,413.7109 \\
7 & -186.5623 &  -58.2018 &  -419.3694 &  -25,413.7715 \\
8 & -187.4427 &  -58.0003 &  -419.4441 &  -25,413.8457 \\
Standard deviation &  11.7621 &  2.9412 &  0.0775 &  0.0775 \\
\hline
\end{tabularx}
\end{table}

While the individual atomic energies fluctuate across models, the sum of atomic energies remains tightly constrained, with a standard deviation two orders of magnitude lower than that of the individual atomic contributions.
This covariance, defined in Eqn. \ref{eq:covariance}, reflects a learned compensation mechanism where certain atomic energy predictions systematically adjust in response to others, captured by deviations of each variable from its expected value.

\begin{equation}
    \label{eq:covariance}
    \text{cov}(x, y) = \langle (x - \langle x \rangle)(y - \langle y \rangle) \rangle
\end{equation}

This suggests that ANI models learn relative atomic energy distributions, where variations in one atom's contribution are counterbalanced by adjustments in others to maintain a stable total molecular energy. 
These covariance effects contribute to the observed size dependence in ANI model uncertainty, where molecular energy predictions remain stable while per-atom uncertainties fluctuate.
This behavior is due to learned covariances across networks, where, for example, carbon atomic energies learn to offset hydrogen energy contributions in the training process. This relationship is defined in Equation \ref{eq:total_e_covariance}.

\begin{equation}
    \sigma_{E_{\text{total}}}^2 = \sum_i^{N \text{ (atoms)}} \sigma_{E_{\text{atomic}}}^2 + 2 \sum_i^N \sum_{j \neq i}^N \text{cov}(x_i, y_j) = \sum_{i,j}^N \text{cov}(x_i, x_j)
    \label{eq:total_e_covariance}
\end{equation}

This formulation of the components in predictive uncertainty by ANI neural networks highlights that the variance $\left( \sigma^2 \right)$ in total molecular energy depends not only on the variance in individual atomic energy contributions, but also the covariances between predictions of different atom types.
The variance in total energy prediction can be expressed as a sum over the covariance matrix, as shown on the right-hand side of Equation \ref{eq:total_e_covariance}. 
Also note that in Table \ref{tbl:ch4_AEs} the low variance in  total energy ($\sigma^2_{\text{total energy}}$) can be explained by the negative covariances between hydrogen and carbon atoms.

To better understand this effect, Figure \ref{fig:ch4_covariance} presents covariance matrices of atomic energy contributions for minimized (using the ANI2xr-ASE interface with the LBFGS \cite{LBFGS} optimizer) structures of methane (CH\textsubscript{4}) and neopentane (C\textsubscript{5}H\textsubscript{12}), highlighting correlation patterns between atomic contributions in molecules of different sizes.

\begin{flushleft}
\begin{multiFigure}
    \addFigure{0.2}{Images/covariance/ch4_labeled.png}
    \addFigure{0.65}{Images/covariance/ch4_covariance.png}
    \addFigure{0.28}{Images/covariance/c5h12_labeled}
    \addFigure{0.65}{Images/covariance/c5h12_covariance.png} \\
\captionof{figure}[Atomic energy covariance matrices]{
(A) Geometry-optimized structure of CH\textsubscript{4}; 
(B) Matrix visualizing the covariance of atomic energies predicted by ANI-2xr for this structure of CH\textsubscript{4}; 
(C) Structure of geometry-optimized C\textsubscript{5}H\textsubscript{12}; 
(D) Covariance matrix for predicted atomic energies of C\textsubscript{5}H\textsubscript{12}.}
\label{fig:ch4_covariance}
\end{multiFigure}
\end{flushleft}

Here, the variance in predicted total energy for methane and neopentane can be computed as a sum over all elements in the matrices shown in Figure \ref{fig:ch4_covariance} (B) and (D).
Note that the covariance between atoms of the same type, for example carbon-carbon, tends to be a large positive number, which is offset by the negative covariances between atoms of different types.
This compensatory behavior suggests that ANI models do not learn absolute atomic energies in isolation, but rather relative energy distributions that maintain a consistent total molecular energy. 
As a result, variations in one atomic contribution are often counterbalanced by adjustments in neighboring atoms, preserving overall energetic stability while allowing individual atomic predictions to fluctuate.

Covariance-driven redistribution of atomic energies is a key challenge in using ANI for uncertainty-aware predictions in inference, and contributes to the observed size dependence in ANI model uncertainty where molecular energy predictions remain stable while per-atom uncertainties fluctuate.
Further, this brings into question the reliability of variance in atomic energy contributions as a viable metric for predictive uncertainty.

\section{Need for a Practical, Physical Quantity to Estimate Uncertainty}
\label{sec:practical_physical_quantity_for_uncertainty}

The search for improvements to measuring predictive uncertainty in ANI models has thus far been examined through distributions in atomic energy prediction, but these values lack intrinsic physical meaning beyond their role in reproducing the total molecular energy. 
A fundamental limitation of atomic energy assignments is their model dependence—each of the eight neural networks in the ensemble is initialized with random parameters to start training and learns a slightly different decomposition of molecular energy, leading to inconsistencies in per-atom contributions. 
This variability makes atomic energies unreliable as a measure of uncertainty, as they do not correspond to an independently verifiable physical property.
A more robust alternative lies in atomic forces, which ANI models also predict as derivatives of the potential energy surface, shown in Equation \ref{eq:force_dE}.

\begin{equation}
    \vec{F} = -\nabla_{\vec{R}} E
    \label{eq:force_dE}
\end{equation}

Unlike atomic energies, forces directly correspond to the physical structure and motion of molecules, as they describe how atoms interact under the influence of a potential. 
Forces dictate molecular geometry, vibrational frequencies, reaction pathways, and dynamics, making them a natural choice for evaluating model reliability.
ANI is trained to approximate the ground-truth \textit{ab initio} PES, its force predictions serve as an indirect measure of how well the model captures the curvature of the PES, providing insight into both local stability and model confidence.
The energy and force loss function used in ANI training, given in Equation \ref{eq:force_loss}, pushes the atomic neural networks to minimize the molecular energy error while also penalizing discrepancies in predicted atomic forces (which are compared component-wise across each vector). 

\begin{equation}
\mathcal{L}_{\text{E \& }\vec{\text{F}}} = 
\frac{1}{N_{\text{M}}} 
\sum_{i=1}^{N_\text{M}} 
\left[ \left( \frac{
\left( E_{\text{ANI}}^{\text{o},i} - E_{\text{QM}}^{\text{o},i} \right)^2}
{\sqrt{N_{\text{atoms}}}} \right)
+ 0.1 \ast \left( 
\frac{\sum_{j=1}^{N_{\text{atoms}}} 
\left( \vec{f}_{\text{ANI}}^{j} - \vec{f}_{\text{QM}}^{j}\right)^2}{N_{\text{atoms}}} 
\right) \right]
\label{eq:force_loss}
\end{equation}

This is based on the loss function showed earlier in Equation \ref{eq:energy_loss_function}, with an additional term that pushes the model to mimic the shape of the PES more closely to the ground-truth surface.
To balance the contributions of energy and force errors, a scaling factor of 0.1 is included, preventing the force predictions from dominating the learning process. 
This ensures that the model refines both energy and force predictions simultaneously, leading to a more physically meaningful representation of molecular interactions.

By training ANI to predict forces along with energies, we gain access to a physically meaningful property for uncertainty estimation. 
Since forces describe how gradients of the potential energy surface vary across atomic positions, their accuracy directly reflects the model's ability to approximate quantum mechanical interactions.
By analyzing the uncertainty in force predictions across the ANI ensemble, we can construct a size-independent measure of model uncertainty that is directly tied to physical observables rather than arbitrary atomic energy assignments.
In the next chapter, we explore how force-based uncertainty metrics can be used to assess ANI model reliability in molecular simulations, and the application of those measures to sampling new data for active learning protocols to expand the training datasets.